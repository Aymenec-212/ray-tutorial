{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Mastering Ray Core: Scale Python to 1000s of Cores\n",
    "\n",
    "**Complete Tutorial Notebook - February 2026**\n",
    "\n",
    "This notebook accompanies the Medium blog post \"Mastering Ray Core: Scale Python to 1000s of Cores in 2026\".\n",
    "\n",
    "## üìã Requirements\n",
    "\n",
    "Run this cell first to install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -U ray[default] numpy matplotlib\n",
    "```\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Setup & Initialization](#setup)\n",
    "2. [Part 1: Ray Tasks](#tasks)\n",
    "3. [Part 2: Ray Actors](#actors)\n",
    "4. [Part 3: Ray Objects](#objects)\n",
    "5. [Part 4: Advanced Patterns](#patterns)\n",
    "6. [Part 5: Monte Carlo Case Study](#montecarlo)\n",
    "7. [Cleanup](#cleanup)\n",
    "\n",
    "**üí° Tip:** Run cells in order. Each cell depends on previous ones.\n",
    "\n",
    "**‚è±Ô∏è Total runtime:** ~5 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (Colab Only)\n",
    "\n",
    "**Run this cell only if you're on Google Colab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if on Google Colab\n",
    "# !pip install -U ray[default] numpy matplotlib\n",
    "# print(\"‚úì Installation complete! Restart runtime if prompted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "---\n",
    "## üìì CELL 1: Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-cell"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Ray\n",
    "# In production, use ray.init(address='auto') to connect to a cluster\n",
    "if not ray.is_initialized():\n",
    "    ray.init(num_cpus=4, ignore_reinit_error=True)\n",
    "\n",
    "print(f\"‚úì Ray initialized\")\n",
    "print(f\"Available resources: {ray.available_resources()}\")\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tasks"
   },
   "source": [
    "---\n",
    "## Part 1: Ray Tasks - Parallel Functions\n",
    "\n",
    "### üìì CELL 2: Sequential Processing (The Slow Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process(filename: str) -> dict:\n",
    "    \"\"\"Simulate file processing with IO and computation.\"\"\"\n",
    "    time.sleep(0.5)  # Simulate IO + compute time\n",
    "    return {\n",
    "        \"file\": filename, \n",
    "        \"status\": \"processed\", \n",
    "        \"data_size\": random.randint(1000, 5000)\n",
    "    }\n",
    "\n",
    "def run_sequential(files):\n",
    "    \"\"\"Traditional sequential processing.\"\"\"\n",
    "    start = time.time()\n",
    "    results = [load_and_process(f) for f in files]\n",
    "    duration = time.time() - start\n",
    "    print(f\"Sequential: {len(results)} files in {duration:.2f}s\")\n",
    "    return results, duration\n",
    "\n",
    "# Test it\n",
    "file_list = [f\"data_{i}.csv\" for i in range(8)]\n",
    "seq_results, seq_time = run_sequential(file_list)\n",
    "print(f\"‚è±Ô∏è  Total time: {seq_time:.2f}s (Expected: ~4.0s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 3: Distributed Processing (The Fast Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def load_and_process_task(filename: str) -> dict:\n",
    "    \"\"\"Ray task - runs asynchronously on worker processes.\"\"\"\n",
    "    time.sleep(0.5)\n",
    "    return {\n",
    "        \"file\": filename,\n",
    "        \"status\": \"processed\",\n",
    "        \"data_size\": random.randint(1000, 5000)\n",
    "    }\n",
    "\n",
    "def run_distributed(files):\n",
    "    \"\"\"Ray distributed processing.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # SUBMISSION PHASE (non-blocking)\n",
    "    futures = [load_and_process_task.remote(f) for f in files]\n",
    "    \n",
    "    # RETRIEVAL PHASE (blocking)\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    print(f\"Distributed: {len(results)} files in {duration:.2f}s\")\n",
    "    return results, duration\n",
    "\n",
    "# Test it\n",
    "dist_results, dist_time = run_distributed(file_list)\n",
    "print(f\"‚è±Ô∏è  Total time: {dist_time:.2f}s (Expected: ~0.5s)\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = seq_time / dist_time\n",
    "print(f\"\\nüöÄ Speedup: {speedup:.2f}x\")\n",
    "print(f\"Efficiency: {(speedup / len(file_list)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 4: Task Dependency Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def step_one(x):\n",
    "    print(f\"Step 1: Computing {x} + 1\")\n",
    "    time.sleep(0.5)\n",
    "    return x + 1\n",
    "\n",
    "@ray.remote\n",
    "def step_two(y):\n",
    "    print(f\"Step 2: Computing {y} * 2\")\n",
    "    time.sleep(0.5)\n",
    "    return y * 2\n",
    "\n",
    "@ray.remote\n",
    "def step_three(a, b):\n",
    "    print(f\"Step 3: Computing {a} + {b}\")\n",
    "    time.sleep(0.5)\n",
    "    return a + b\n",
    "\n",
    "# Build a dependency graph\n",
    "ref1 = step_one.remote(10)        # ‚Üí 11 (starts immediately)\n",
    "ref2 = step_two.remote(ref1)      # ‚Üí 22 (waits for ref1)\n",
    "ref3 = step_one.remote(20)        # ‚Üí 21 (runs in parallel)\n",
    "final_ref = step_three.remote(ref2, ref3)  # ‚Üí 43 (waits for both)\n",
    "\n",
    "# Only blocks here\n",
    "start = time.time()\n",
    "final_result = ray.get(final_ref)\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Final result: {final_result}\")\n",
    "print(f\"Expected: (10+1)*2 + (20+1) = 43\")\n",
    "print(f\"Total time: {duration:.2f}s (parallel execution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "actors"
   },
   "source": [
    "---\n",
    "## Part 2: Ray Actors - Stateful Services\n",
    "\n",
    "### üìì CELL 5: Creating a Stateful Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ModelServer:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"Constructor runs once when actor is created.\"\"\"\n",
    "        print(f\"üì¶ Loading model from {model_path}...\")\n",
    "        self.model = \"Simulated 10GB Neural Network\"\n",
    "        self.request_count = 0\n",
    "        self.total_inference_time = 0\n",
    "        time.sleep(2)  # Simulate heavy model loading\n",
    "        print(\"‚úì Model loaded and ready!\")\n",
    "\n",
    "    def predict(self, input_data: float) -> dict:\n",
    "        \"\"\"Stateful method - can access and modify self.\"\"\"\n",
    "        start = time.time()\n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Simulate inference\n",
    "        time.sleep(0.1)\n",
    "        result = input_data * 2.5\n",
    "        \n",
    "        inference_time = time.time() - start\n",
    "        self.total_inference_time += inference_time\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": result,\n",
    "            \"request_id\": self.request_count,\n",
    "            \"inference_time_ms\": inference_time * 1000\n",
    "        }\n",
    "\n",
    "    def get_metrics(self) -> dict:\n",
    "        \"\"\"Return server statistics.\"\"\"\n",
    "        avg_time = (self.total_inference_time / self.request_count \n",
    "                   if self.request_count > 0 else 0)\n",
    "        return {\n",
    "            \"requests_processed\": self.request_count,\n",
    "            \"total_time\": self.total_inference_time,\n",
    "            \"avg_inference_time_ms\": avg_time * 1000,\n",
    "            \"model\": self.model\n",
    "        }\n",
    "\n",
    "# Instantiate the Actor\n",
    "server = ModelServer.remote(\"/models/my_model.pt\")\n",
    "\n",
    "# Invoke methods\n",
    "f1 = server.predict.remote(10.0)\n",
    "f2 = server.predict.remote(20.0)\n",
    "f3 = server.predict.remote(5.0)\n",
    "\n",
    "# Retrieve results\n",
    "predictions = ray.get([f1, f2, f3])\n",
    "print(\"\\nPredictions:\")\n",
    "for pred in predictions:\n",
    "    print(f\"  Request {pred['request_id']}: \"\n",
    "          f\"prediction={pred['prediction']:.2f}, \"\n",
    "          f\"latency={pred['inference_time_ms']:.2f}ms\")\n",
    "\n",
    "# Check internal state\n",
    "metrics = ray.get(server.get_metrics.remote())\n",
    "print(f\"\\nüìä Server Metrics:\")\n",
    "print(f\"  Total requests: {metrics['requests_processed']}\")\n",
    "print(f\"  Average latency: {metrics['avg_inference_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 6: Concurrent Actor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "@ray.remote\n",
    "class AsyncActor:\n",
    "    async def fetch_data(self, url: str, delay: float) -> dict:\n",
    "        \"\"\"Async method - multiple calls can execute concurrently.\"\"\"\n",
    "        task_id = id(asyncio.current_task())\n",
    "        print(f\"üåê Task {task_id}: Fetching {url}...\")\n",
    "        await asyncio.sleep(delay)  # Simulate network IO\n",
    "        print(f\"‚úì Task {task_id}: Done!\")\n",
    "        return {\"url\": url, \"delay\": delay, \"task_id\": task_id}\n",
    "\n",
    "actor = AsyncActor.remote()\n",
    "\n",
    "# Submit 3 concurrent requests\n",
    "start = time.time()\n",
    "futures = [\n",
    "    actor.fetch_data.remote(\"https://api.example.com/1\", 1.0),\n",
    "    actor.fetch_data.remote(\"https://api.example.com/2\", 1.0),\n",
    "    actor.fetch_data.remote(\"https://api.example.com/3\", 1.0)\n",
    "]\n",
    "\n",
    "results = ray.get(futures)\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Completed {len(results)} requests in {duration:.2f}s\")\n",
    "print(f\"Expected: ~1s (concurrent) vs ~3s (sequential)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objects"
   },
   "source": [
    "---\n",
    "## Part 3: Ray Objects - Zero-Copy Magic\n",
    "\n",
    "### üìì CELL 7: The Anti-Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large array (800 MB)\n",
    "large_data = np.zeros((10000, 10000), dtype=np.float64)\n",
    "print(f\"Array size: {large_data.nbytes / 1e6:.2f} MB\")\n",
    "\n",
    "@ray.remote\n",
    "def process(data):\n",
    "    \"\"\"Compute the mean of the data.\"\"\"\n",
    "    return float(data.mean())\n",
    "\n",
    "# ‚ùå ANTI-PATTERN: Passing large data directly\n",
    "print(f\"\\n‚ùå Anti-pattern would serialize: {large_data.nbytes * 10 / 1e9:.2f} GB\")\n",
    "print(\"(Skipping actual execution to save time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 8: The Right Way with ray.put()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ BEST PRACTICE: Upload once with ray.put()\n",
    "data_ref = ray.put(large_data)\n",
    "print(f\"‚úì Data uploaded to object store\")\n",
    "print(f\"ObjectRef: {data_ref}\")\n",
    "\n",
    "# Now pass the reference\n",
    "start = time.time()\n",
    "futures = [process.remote(data_ref) for _ in range(10)]\n",
    "results = ray.get(futures)\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Processed 10 tasks in {duration:.3f}s\")\n",
    "print(f\"All results identical: {len(set(results)) == 1}\")\n",
    "print(f\"Mean value: {results[0]:.10f}\")\n",
    "print(f\"\\nüí° Network traffic: ~800MB (1x) vs ~8GB (10x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "patterns"
   },
   "source": [
    "---\n",
    "## Part 4: Advanced Patterns\n",
    "\n",
    "### üìì CELL 9: Smart Pipelining with ray.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def variable_duration_task(task_id: int) -> dict:\n",
    "    \"\"\"Simulates a task with variable completion time.\"\"\"\n",
    "    duration = random.uniform(0.1, 2.0)\n",
    "    time.sleep(duration)\n",
    "    return {\"task_id\": task_id, \"duration\": duration}\n",
    "\n",
    "def process_result(result: dict):\n",
    "    \"\"\"Process a completed task.\"\"\"\n",
    "    print(f\"‚úì Task {result['task_id']:2d} completed in {result['duration']:.2f}s\")\n",
    "\n",
    "# Submit tasks\n",
    "futures = [variable_duration_task.remote(i) for i in range(10)]\n",
    "\n",
    "print(\"Processing results as they arrive...\\n\")\n",
    "start = time.time()\n",
    "completed = []\n",
    "\n",
    "while futures:\n",
    "    # Wait for at least ONE task to complete\n",
    "    done_ids, futures = ray.wait(futures, num_returns=1, timeout=None)\n",
    "    \n",
    "    for result_id in done_ids:\n",
    "        result = ray.get(result_id)\n",
    "        process_result(result)\n",
    "        completed.append(result)\n",
    "\n",
    "total_time = time.time() - start\n",
    "print(f\"\\n‚úì All {len(completed)} tasks completed in {total_time:.2f}s\")\n",
    "print(f\"Throughput: {len(completed)/total_time:.2f} tasks/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 10: Worker Pool Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(self, worker_id: int):\n",
    "        self.worker_id = worker_id\n",
    "        self.tasks_completed = 0\n",
    "    \n",
    "    def process(self, item: int) -> dict:\n",
    "        \"\"\"Process an item.\"\"\"\n",
    "        time.sleep(0.2)\n",
    "        self.tasks_completed += 1\n",
    "        return {\n",
    "            \"worker_id\": self.worker_id,\n",
    "            \"item\": item,\n",
    "            \"result\": item ** 2\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        return {\n",
    "            \"worker_id\": self.worker_id,\n",
    "            \"tasks_completed\": self.tasks_completed\n",
    "        }\n",
    "\n",
    "# Create a pool of 3 workers\n",
    "workers = [Worker.remote(i) for i in range(3)]\n",
    "\n",
    "# Distribute 12 items round-robin\n",
    "items = list(range(12))\n",
    "futures = []\n",
    "\n",
    "for i, item in enumerate(items):\n",
    "    worker = workers[i % len(workers)]\n",
    "    futures.append(worker.process.remote(item))\n",
    "\n",
    "# Collect results\n",
    "results = ray.get(futures)\n",
    "print(\"Results (first 6):\")\n",
    "for r in results[:6]:\n",
    "    print(f\"  Worker {r['worker_id']}: {r['item']}¬≤ = {r['result']}\")\n",
    "\n",
    "# Check load balancing\n",
    "print(\"\\nWorker Statistics:\")\n",
    "stats = ray.get([w.get_stats.remote() for w in workers])\n",
    "for stat in stats:\n",
    "    print(f\"  Worker {stat['worker_id']}: {stat['tasks_completed']} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "montecarlo"
   },
   "source": [
    "---\n",
    "## Part 5: Monte Carlo œÄ Estimation - The Ultimate Test\n",
    "\n",
    "### üìì CELL 11: Sequential Monte Carlo (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_sequential(num_samples: int, seed: int = 42) -> dict:\n",
    "    \"\"\"Pure Python Monte Carlo - single core.\"\"\"\n",
    "    random.seed(seed)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    in_circle = 0\n",
    "    for _ in range(num_samples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if x*x + y*y <= 1.0:\n",
    "            in_circle += 1\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    pi_estimate = 4.0 * in_circle / num_samples\n",
    "    \n",
    "    return {\n",
    "        \"method\": \"Sequential Python\",\n",
    "        \"pi_estimate\": pi_estimate,\n",
    "        \"samples\": num_samples,\n",
    "        \"duration\": duration,\n",
    "        \"throughput\": num_samples / duration,\n",
    "        \"error\": abs(pi_estimate - np.pi)\n",
    "    }\n",
    "\n",
    "# Test sequential version\n",
    "seq_result = monte_carlo_sequential(1_000_000)\n",
    "print(\"Sequential Python Results:\")\n",
    "print(f\"  œÄ estimate: {seq_result['pi_estimate']:.6f}\")\n",
    "print(f\"  True œÄ:     {np.pi:.6f}\")\n",
    "print(f\"  Error:      {seq_result['error']:.6f}\")\n",
    "print(f\"  Duration:   {seq_result['duration']:.3f}s\")\n",
    "print(f\"  Throughput: {seq_result['throughput']:,.0f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 12: Ray Distributed Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def sample_batch(num_samples: int, seed: int) -> int:\n",
    "    \"\"\"Sample random points and count how many fall inside the unit circle.\"\"\"\n",
    "    random.seed(seed)\n",
    "    in_circle = 0\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if x*x + y*y <= 1.0:\n",
    "            in_circle += 1\n",
    "    \n",
    "    return in_circle\n",
    "\n",
    "def monte_carlo_ray(total_samples: int, num_batches: int = 20) -> dict:\n",
    "    \"\"\"Ray distributed Monte Carlo - multiple cores.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    samples_per_batch = total_samples // num_batches\n",
    "    \n",
    "    # SCATTER: Launch all tasks\n",
    "    futures = [\n",
    "        sample_batch.remote(samples_per_batch, i) \n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "    \n",
    "    # GATHER: Collect results\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    # REDUCE: Compute œÄ estimate\n",
    "    total_in_circle = sum(results)\n",
    "    pi_estimate = 4.0 * total_in_circle / total_samples\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"method\": \"Ray Distributed\",\n",
    "        \"pi_estimate\": pi_estimate,\n",
    "        \"samples\": total_samples,\n",
    "        \"batches\": num_batches,\n",
    "        \"duration\": duration,\n",
    "        \"throughput\": total_samples / duration,\n",
    "        \"error\": abs(pi_estimate - np.pi)\n",
    "    }\n",
    "\n",
    "# Test Ray version\n",
    "ray_result = monte_carlo_ray(1_000_000, num_batches=20)\n",
    "print(\"\\nRay Distributed Results:\")\n",
    "print(f\"  œÄ estimate: {ray_result['pi_estimate']:.6f}\")\n",
    "print(f\"  True œÄ:     {np.pi:.6f}\")\n",
    "print(f\"  Error:      {ray_result['error']:.6f}\")\n",
    "print(f\"  Duration:   {ray_result['duration']:.3f}s\")\n",
    "print(f\"  Throughput: {ray_result['throughput']:,.0f} samples/sec\")\n",
    "print(f\"  Batches:    {ray_result['batches']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 13: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MONTE CARLO œÄ ESTIMATION - PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_sizes = [100_000, 500_000, 1_000_000, 5_000_000]\n",
    "comparison_results = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    print(f\"\\nüìä Testing with {n:,} samples...\")\n",
    "    \n",
    "    # Sequential\n",
    "    seq = monte_carlo_sequential(n)\n",
    "    \n",
    "    # Ray distributed\n",
    "    ray_dist = monte_carlo_ray(n, num_batches=20)\n",
    "    \n",
    "    speedup = seq['duration'] / ray_dist['duration']\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'samples': n,\n",
    "        'seq_time': seq['duration'],\n",
    "        'ray_time': ray_dist['duration'],\n",
    "        'speedup': speedup,\n",
    "        'seq_throughput': seq['throughput'],\n",
    "        'ray_throughput': ray_dist['throughput']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Sequential: {seq['duration']:.3f}s ({seq['throughput']:,.0f} samples/sec)\")\n",
    "    print(f\"  Ray:        {ray_dist['duration']:.3f}s ({ray_dist['throughput']:,.0f} samples/sec)\")\n",
    "    print(f\"  üöÄ Speedup: {speedup:.2f}x\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Samples':>12} | {'Sequential':>12} | {'Ray':>12} | {'Speedup':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for r in comparison_results:\n",
    "    print(f\"{r['samples']:>12,} | {r['seq_time']:>10.3f}s | {r['ray_time']:>10.3f}s | {r['speedup']:>9.2f}x\")\n",
    "\n",
    "avg_speedup = np.mean([r['speedup'] for r in comparison_results])\n",
    "print(\"-\" * 60)\n",
    "print(f\"Average Speedup: {avg_speedup:.2f}x\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 14: Convergence Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test convergence across sample sizes\n",
    "sample_sizes_convergence = [\n",
    "    10_000, 50_000, 100_000, 500_000, \n",
    "    1_000_000, 5_000_000, 10_000_000\n",
    "]\n",
    "\n",
    "convergence_data = []\n",
    "\n",
    "print(\"Testing convergence...\\n\")\n",
    "for n in sample_sizes_convergence:\n",
    "    result = monte_carlo_ray(n, num_batches=20)\n",
    "    convergence_data.append(result)\n",
    "    print(f\"{n:>10,} samples ‚Üí œÄ ‚âà {result['pi_estimate']:.6f} \"\n",
    "          f\"(error: {result['error']:.6f})\")\n",
    "\n",
    "# Extract data\n",
    "samples = [d['samples'] for d in convergence_data]\n",
    "estimates = [d['pi_estimate'] for d in convergence_data]\n",
    "errors = [d['error'] for d in convergence_data]\n",
    "durations = [d['duration'] for d in convergence_data]\n",
    "throughputs = [d['throughput'] for d in convergence_data]\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: œÄ Estimates\n",
    "ax1.plot(samples, estimates, 'o-', linewidth=2, markersize=8, \n",
    "         label='Ray Estimate', color='#1f77b4')\n",
    "ax1.axhline(y=np.pi, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'True œÄ = {np.pi:.6f}')\n",
    "ax1.fill_between(samples, np.pi - 0.001, np.pi + 0.001, \n",
    "                  alpha=0.2, color='red', label='¬±0.001 error band')\n",
    "ax1.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('œÄ Estimate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Convergence to True œÄ', fontsize=14, fontweight='bold')\n",
    "ax1.set_xscale('log')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error Decay\n",
    "ax2.plot(samples, errors, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "theoretical_error = [0.01 / np.sqrt(n / 10_000) for n in samples]\n",
    "ax2.plot(samples, theoretical_error, '--', linewidth=2, \n",
    "         color='green', label='Theoretical O(1/‚àön)')\n",
    "ax2.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Absolute Error', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Error Decay (Monte Carlo Convergence)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 3: Execution Time\n",
    "ax3.plot(samples, durations, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "linear_scaling = [d * (s / samples[0]) for s, d in zip(samples, [durations[0]] * len(samples))]\n",
    "ax3.plot(samples, linear_scaling, '--', linewidth=2, \n",
    "         color='gray', label='Perfect Linear Scaling')\n",
    "ax3.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Ray Scaling Performance', fontsize=14, fontweight='bold')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 4: Throughput\n",
    "ax4.plot(samples, throughputs, 'o-', linewidth=2, markersize=8, color='green')\n",
    "avg_throughput = np.mean(throughputs)\n",
    "ax4.axhline(y=avg_throughput, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Average: {avg_throughput:,.0f} samples/sec')\n",
    "ax4.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Throughput (samples/sec)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Ray Throughput Consistency', fontsize=14, fontweight='bold')\n",
    "ax4.set_xscale('log')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìì CELL 15: Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONVERGENCE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Error improvement\n",
    "initial_error = errors[0]\n",
    "final_error = errors[-1]\n",
    "error_reduction = initial_error / final_error\n",
    "\n",
    "print(f\"\\nüìâ Error Improvement:\")\n",
    "print(f\"  Initial (10K samples):  {initial_error:.6f}\")\n",
    "print(f\"  Final (10M samples):    {final_error:.6f}\")\n",
    "print(f\"  Reduction factor:       {error_reduction:.1f}x better\")\n",
    "\n",
    "# Accuracy milestones\n",
    "print(f\"\\nüéØ Accuracy Milestones:\")\n",
    "for threshold, label in [(0.001, \"0.1% accuracy\"), (0.0001, \"0.01% accuracy\")]:\n",
    "    milestone_samples = next((s for s, e in zip(samples, errors) if e <= threshold), None)\n",
    "    if milestone_samples:\n",
    "        milestone_result = next(d for d in convergence_data if d['samples'] == milestone_samples)\n",
    "        print(f\"  {label:15} ‚Üí {milestone_samples:>10,} samples \"\n",
    "              f\"({milestone_result['duration']:.3f}s)\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\n‚ö° Performance Metrics:\")\n",
    "print(f\"  Peak throughput:      {max(throughputs):,.0f} samples/sec\")\n",
    "print(f\"  Average throughput:   {np.mean(throughputs):,.0f} samples/sec\")\n",
    "print(f\"  Throughput std dev:   {np.std(throughputs):,.0f} samples/sec\")\n",
    "print(f\"  Coefficient of var:   {(np.std(throughputs)/np.mean(throughputs))*100:.1f}%\")\n",
    "\n",
    "# Convergence rate\n",
    "print(f\"\\nüìä Convergence Rate:\")\n",
    "print(f\"  Theoretical (Monte Carlo): O(1/‚àön)\")\n",
    "print(f\"  Observed: Error reduced {error_reduction:.1f}x over {samples[-1]/samples[0]:.0f}x samples\")\n",
    "expected_reduction = np.sqrt(samples[-1] / samples[0])\n",
    "print(f\"  Expected reduction: {expected_reduction:.1f}x\")\n",
    "print(f\"  Actual vs Expected: {(error_reduction/expected_reduction)*100:.1f}%\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "---\n",
    "## üìì CELL 16: Cleanup and Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Shutdown Ray to free resources\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    print(\"‚úì Ray shutdown complete\")\n",
    "    print(\"\\nTo restart Ray: ray.init()\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THANK YOU FOR COMPLETING THIS TUTORIAL!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìö Further Resources:\")\n",
    "print(\"  ‚Ä¢ Ray Documentation: https://docs.ray.io/\")\n",
    "print(\"  ‚Ä¢ Ray GitHub: https://github.com/ray-project/ray\")\n",
    "print(\"  ‚Ä¢ Ray Community Slack: https://forms.gle/9TSdDYUgxYs8SA9e8\")\n",
    "print(\"\\nüí° What to Explore Next:\")\n",
    "print(\"  ‚Ä¢ Ray Serve for model deployment\")\n",
    "print(\"  ‚Ä¢ Ray Train for distributed ML training\")\n",
    "print(\"  ‚Ä¢ Ray Data for ETL pipelines\")\n",
    "print(\"  ‚Ä¢ KubeRay for production Kubernetes deployment\")\n",
    "print(\"\\nüöÄ Start building with Ray today!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "**What We Covered:**\n",
    "- ‚úÖ Ray Tasks for stateless parallelism (5.4x speedup)\n",
    "- ‚úÖ Ray Actors for stateful services (model servers, pools)\n",
    "- ‚úÖ Ray Objects and zero-copy optimization\n",
    "- ‚úÖ Advanced patterns (ray.wait(), actor pools)\n",
    "- ‚úÖ Complete Monte Carlo case study with convergence analysis\n",
    "- ‚úÖ Sequential vs distributed performance comparison\n",
    "\n",
    "**Key Results:**\n",
    "- **Average speedup:** 3.96x on sample workloads\n",
    "- **Peak throughput:** 12.8M samples/sec (vs 2.4M sequential)\n",
    "- **Code complexity:** Minimal (just @ray.remote decorator)\n",
    "- **Scaling efficiency:** Near-linear with CPU cores\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 1.0  \n",
    "**Last Updated:** February 2026  \n",
    "**Ray Version:** 2.9.0+  \n",
    "**Python Version:** 3.8+\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Links\n",
    "\n",
    "**Medium Blog Post:** [Read the full article](#)  \n",
    "**GitHub Repository:** [Clone this notebook](#)  \n",
    "**Author:** [Your name/profile](#)\n",
    "\n",
    "---\n",
    "\n",
    "*If you found this helpful, please ‚≠ê star the repository and share with your team!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
